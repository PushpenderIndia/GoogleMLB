{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10652824,"sourceType":"datasetVersion","datasetId":6581138}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Overview of MLB Homeruns Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nmlb_homeruns_df = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2016-mlb-homeruns.csv')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T04:19:00.209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mlb_homeruns_df.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T04:19:00.209Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing & Training in Batches","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nimport requests\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, LSTM, TimeDistributed\nfrom tensorflow.keras.optimizers import Adam\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom tqdm import tqdm\nimport tensorflow.keras.backend as K\nimport gc\n\n# Load pre-trained CNN model (feature extractor)\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nbase_model.trainable = False  # Freeze pre-trained layers\nfeature_extractor = Model(inputs=base_model.input, outputs=Flatten()(base_model.output))  # Extracted features\n\n# Define the LSTM-based regression model\nmodel = Sequential([\n    TimeDistributed(feature_extractor, input_shape=(50, 224, 224, 3)),  # Apply CNN to each frame\n    LSTM(256, return_sequences=False),  # LSTM processes extracted features\n    Dense(512, activation='relu'),\n    Dropout(0.3),\n    Dense(3)  # Predict ExitVelocity, HitDistance, LaunchAngle\n])\n\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n\n# Function to download video\ndef download_video(video_url, save_path):\n    response = requests.get(video_url, stream=True)\n    with open(save_path, 'wb') as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            file.write(chunk)\n    return save_path\n\n# Function to extract frames from video\ndef preprocess_video(video_url, save_path, frame_count=50):\n    video_path = download_video(video_url, f'{save_path}.mp4')\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_ids = np.linspace(0, total_frames - 1, frame_count, dtype=int)  # Sample evenly\n    \n    for fid in frame_ids:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, fid)\n        ret, frame = cap.read()\n        if not ret:\n            continue\n        frame = cv2.resize(frame, (224, 224)) / 255.0  # Normalize\n        frames.append(frame)\n    \n    cap.release()\n    os.remove(video_path)  # Remove temp file\n    return np.array(frames) if len(frames) == frame_count else None  # Ensure 10 frames\n\n# Load and merge datasets\ndf1 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2016-mlb-homeruns.csv')\ndf2 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2017-mlb-homeruns.csv')\ndf3 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2024-mlb-homeruns.csv')\ndf = pd.concat([df1, df2, df3], ignore_index=True)\n\n# Prepare training data\nbatch_size = 50\nnum_batches = len(df) // batch_size + (len(df) % batch_size != 0)\n\n# Create temp directory for videos\nos.makedirs(\"temp_videos\", exist_ok=True)\n\nfor batch_idx in range(num_batches):\n    print(f\"Processing batch {batch_idx + 1}/{num_batches}\")\n    df_batch = df[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n    X, y = [], []\n    \n    # Process videos with proper tqdm progress tracking\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        futures = {executor.submit(preprocess_video, row['video'], 'temp_videos/' + row['play_id']): row for _, row in df_batch.iterrows()}\n        \n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Videos\"):\n            res = future.result()\n            if res is not None:\n                X.append(res)\n                y.append([futures[future]['ExitVelocity'], futures[future]['HitDistance'], futures[future]['LaunchAngle']])\n    \n    if len(X) == 0:\n        print(\"Skipping batch due to no valid videos.\")\n        continue\n\n    X = np.array(X)  # Shape: (batch_size, 10, 224, 224, 3)\n    y = np.array(y)  # Shape: (batch_size, 3)\n\n    print(\"Split data into train and test\")\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    print(\"Train model\")\n    model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test, y_test))\n\n    print(\"Deleting Previous Saved model\")\n    if os.path.exists(f'model_batch_{batch_idx}.h5'):\n        os.remove(f'model_batch_{batch_idx}.h5')\n    \n    print(\"Save model after each batch\")\n    model.save(f'model_batch_{batch_idx + 1}.h5')\n    print(f\"Model saved after batch {batch_idx + 1}\")\n\n    print(\"Evaluate model\")\n    loss, mae = model.evaluate(X_test, y_test)\n    print(f'Batch {batch_idx + 1} - Test Loss: {loss}, Test MAE: {mae}')\n\n    # Clear the TensorFlow session to release memory\n    K.clear_session()\n    \n    # Trigger garbage collection to free up unused memory\n    gc.collect()\n\n    # Clear variables (X and y data) that are no longer needed\n    X = []\n    y = []\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T04:19:00.209Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Making Prediction by Loading Model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport cv2\nimport os\nimport requests\n\n# Load the trained model\nmodel_path = \"/kaggle/working/model_batch_8.h5\"  # Replace 'X' with the latest batch number\nmodel = tf.keras.models.load_model(model_path, compile=False)\n\n# Recompile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              loss='mse',  # Explicitly set the loss function again\n              metrics=['mae'])\n\n# Function to preprocess new video\ndef preprocess_video(video_url, save_path, frame_count=50):\n    response = requests.get(video_url, stream=True)\n    with open(save_path, 'wb') as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            file.write(chunk)\n\n    cap = cv2.VideoCapture(save_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_ids = np.linspace(0, total_frames - 1, frame_count, dtype=int)\n\n    for fid in frame_ids:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, fid)\n        ret, frame = cap.read()\n        if not ret:\n            continue\n        frame = cv2.resize(frame, (224, 224)) / 255.0  # Normalize\n        frames.append(frame)\n\n    cap.release()\n    os.remove(save_path)  # Remove temporary video file\n\n    if len(frames) == frame_count:\n        return np.array(frames)\n    else:\n        return None  # Return None if video does not have enough frames\n\n# Path to the new video for prediction\nrow_index = 700\nvideo_url = mlb_homeruns_df['video'][row_index]  # Replace with actual video URL\nvideo_save_path = \"temp_video.mp4\"\n\n# Preprocess video\nvideo_frames = preprocess_video(video_url, video_save_path)\n\nif video_frames is not None:\n    video_frames = np.expand_dims(video_frames, axis=0)  # Add batch dimension (1, 50, 224, 224, 3)\n\n    # Make prediction\n    predictions = model.predict(video_frames)\n\n    # Display predicted values\n    print(f\"Predicted ExitVelocity: {predictions[0][0]:.2f}\")\n    print(f\"Predicted HitDistance: {predictions[0][1]:.2f}\")\n    print(f\"Predicted LaunchAngle: {predictions[0][2]:.2f}\")\n\n    print(f\"\\n\\nActual ExitVelocity: {mlb_homeruns_df['ExitVelocity'][row_index]}\")\n    print(f\"Actual HitDistance: {mlb_homeruns_df['HitDistance'][row_index]}\")\n    print(f\"Actual LaunchAngle: {mlb_homeruns_df['LaunchAngle'][row_index]}\")\nelse:\n    print(\"Error: Not enough valid frames in the video.\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T04:19:00.209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mlb_homeruns_df.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-04T04:19:00.209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nimport requests\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, LSTM, TimeDistributed\nfrom tensorflow.keras.optimizers import Adam\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom tqdm import tqdm\nimport tensorflow.keras.backend as K\nimport gc\n\n# Load pre-trained CNN model (feature extractor)\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nbase_model.trainable = False  # Freeze pre-trained layers\nfeature_extractor = Model(inputs=base_model.input, outputs=Flatten()(base_model.output))  # Extracted features\n\n# Define the LSTM-based regression model\nmodel = Sequential([\n    TimeDistributed(feature_extractor, input_shape=(60, 224, 224, 3)),  # Apply CNN to each frame\n    LSTM(256, return_sequences=False),  # LSTM processes extracted features\n    Dense(512, activation='relu'),\n    Dropout(0.3),\n    Dense(3)  # Predict ExitVelocity, HitDistance, LaunchAngle\n])\n\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\nmodel.summary()\n\n# Function to download video\ndef download_video(video_url, save_path):\n    response = requests.get(video_url, stream=True)\n    with open(save_path, 'wb') as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            file.write(chunk)\n    return save_path\n\n# Function to extract frames from video\ndef preprocess_video(video_url, save_path, frame_count=60):\n    video_path = download_video(video_url, f'{save_path}.mp4')\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_ids = np.linspace(0, total_frames - 1, frame_count, dtype=int)  # Sample evenly\n    \n    for fid in frame_ids:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, fid)\n        ret, frame = cap.read()\n        if not ret:\n            continue\n        frame = cv2.resize(frame, (224, 224)) / 255.0  # Normalize\n        frames.append(frame)\n    \n    cap.release()\n    os.remove(video_path)  # Remove temp file\n    return np.array(frames) if len(frames) == frame_count else None \n\n# Load and merge datasets\ndf1 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2016-mlb-homeruns.csv')\ndf2 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2017-mlb-homeruns.csv')\ndf3 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2024-mlb-homeruns.csv')\ndf = pd.concat([df1, df2, df3], ignore_index=True)\n\n# Prepare training data\nbatch_size = 30\nnum_batches = len(df) // batch_size + (len(df) % batch_size != 0)\n\n# Create temp directory for videos\nos.makedirs(\"temp_videos\", exist_ok=True)\n\nfor batch_idx in range(num_batches):\n    print(f\"Processing batch {batch_idx + 1}/{num_batches}\")\n    df_batch = df[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n    X, y = [], []\n    \n    # Process videos with proper tqdm progress tracking\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        futures = {executor.submit(preprocess_video, row['video'], 'temp_videos/' + row['play_id']): row for _, row in df_batch.iterrows()}\n        \n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Videos\"):\n            res = future.result()\n            if res is not None:\n                X.append(res)\n                y.append([futures[future]['ExitVelocity'], futures[future]['HitDistance'], futures[future]['LaunchAngle']])\n    \n    if len(X) == 0:\n        print(\"Skipping batch due to no valid videos.\")\n        continue\n\n    X = np.array(X)  # Shape: (batch_size, 10, 224, 224, 3)\n    y = np.array(y)  # Shape: (batch_size, 3)\n\n    print(\"Split data into train and test\")\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    print(\"Train model\")\n    model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test, y_test))\n\n    print(\"Deleting Previous Saved model\")\n    if os.path.exists(f'model_batch_{batch_idx}.h5'):\n        os.remove(f'model_batch_{batch_idx}.h5')\n    \n    print(\"Save model after each batch\")\n    model.save(f'model_batch_{batch_idx + 1}.h5')\n    print(f\"Model saved after batch {batch_idx + 1}\")\n\n    print(\"Evaluate model\")\n    loss, mae = model.evaluate(X_test, y_test)\n    print(f'Batch {batch_idx + 1} - Test Loss: {loss}, Test MAE: {mae}')\n\n    # Clear the TensorFlow session to release memory\n    K.clear_session()\n    \n    # Trigger garbage collection to free up unused memory\n    gc.collect()\n\n    # Clear variables (X and y data) that are no longer needed\n    X = []\n    y = []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:24:17.958283Z","iopub.execute_input":"2025-02-04T11:24:17.958469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nimport requests\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, LSTM, TimeDistributed\nfrom tensorflow.keras.optimizers import Adam\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom tqdm import tqdm\nimport tensorflow.keras.backend as K\nimport gc\n\n# Load pre-trained CNN model (feature extractor)\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nbase_model.trainable = False  # Freeze pre-trained layers\nfeature_extractor = Model(inputs=base_model.input, outputs=Flatten()(base_model.output))  # Extracted features\n\n# Define the LSTM-based regression model\nmodel = Sequential([\n    TimeDistributed(feature_extractor, input_shape=(5, 224, 224, 3)),  # Apply CNN to each frame\n    LSTM(256, return_sequences=False),  # LSTM processes extracted features\n    Dense(512, activation='relu'),\n    Dropout(0.3),\n    Dense(3)  # Predict ExitVelocity, HitDistance, LaunchAngle\n])\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\nmodel.summary()\n\n# Function to download video\ndef download_video(video_url, save_path):\n    response = requests.get(video_url, stream=True)\n    with open(save_path, 'wb') as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            file.write(chunk)\n    return save_path\n\n# Function to detect ball-and-bat collision\ndef detect_collision(cap):\n    \"\"\"\n    Detects the frame where ball-and-bat collision occurs.\n    This is a placeholder implementation; you may need to use object detection or motion analysis.\n    \"\"\"\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Start from the beginning\n    prev_frame = None\n    collision_frame_id = 0\n\n    for frame_id in range(total_frames):\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Convert frame to grayscale for motion detection\n        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        gray_frame = cv2.GaussianBlur(gray_frame, (21, 21), 0)\n\n        if prev_frame is None:\n            prev_frame = gray_frame\n            continue\n\n        # Compute absolute difference between current and previous frame\n        frame_diff = cv2.absdiff(prev_frame, gray_frame)\n        _, thresh = cv2.threshold(frame_diff, 30, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # If significant motion is detected, assume collision\n        if any(cv2.contourArea(contour) > 500 for contour in contours):  # Adjust threshold as needed\n            collision_frame_id = frame_id\n            break\n\n        prev_frame = gray_frame\n\n    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset to the beginning\n    return collision_frame_id\n\n# Function to extract frames from video\ndef preprocess_video(video_url, save_path, frame_count=5):\n    video_path = download_video(video_url, f'{save_path}.mp4')\n    cap = cv2.VideoCapture(video_path)\n\n    # Detect ball-and-bat collision\n    collision_frame_id = detect_collision(cap)\n    print(f\"Detected collision at frame {collision_frame_id}\")\n\n    # Extract frames starting from the collision frame\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.set(cv2.CAP_PROP_POS_FRAMES, collision_frame_id)  # Start from collision frame\n\n    for _ in range(frame_count):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.resize(frame, (224, 224)) / 255.0  # Normalize\n        frames.append(frame)\n\n    cap.release()\n    os.remove(video_path)  # Remove temp file\n    return np.array(frames) if len(frames) == frame_count else None\n\n# Load and merge datasets\ndf1 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2016-mlb-homeruns.csv')\ndf2 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2017-mlb-homeruns.csv')\ndf3 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2024-mlb-homeruns.csv')\ndf = pd.concat([df1, df2, df3], ignore_index=True)\n\n# Prepare training data\nbatch_size = 30\nnum_batches = len(df) // batch_size + (len(df) % batch_size != 0)\n\n# Create temp directory for videos\nos.makedirs(\"temp_videos\", exist_ok=True)\n\nfor batch_idx in range(num_batches):\n    print(f\"Processing batch {batch_idx + 1}/{num_batches}\")\n    df_batch = df[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n    X, y = [], []\n\n    # Process videos with proper tqdm progress tracking\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        futures = {executor.submit(preprocess_video, row['video'], 'temp_videos/' + row['play_id']): row for _, row in df_batch.iterrows()}\n\n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Videos\"):\n            res = future.result()\n            if res is not None:\n                X.append(res)\n                y.append([futures[future]['ExitVelocity'], futures[future]['HitDistance'], futures[future]['LaunchAngle']])\n\n    if len(X) == 0:\n        print(\"Skipping batch due to no valid videos.\")\n        continue\n\n    X = np.array(X)  # Shape: (batch_size, 15, 224, 224, 3)\n    y = np.array(y)  # Shape: (batch_size, 3)\n\n    print(\"Split data into train and test\")\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    print(\"Train model\")\n    model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test, y_test))\n\n    print(\"Deleting Previous Saved model\")\n    if os.path.exists(f'model_batch_{batch_idx}.h5'):\n        os.remove(f'model_batch_{batch_idx}.h5')\n\n    print(\"Save model after each batch\")\n    model.save(f'model_batch_{batch_idx + 1}.h5')\n    print(f\"Model saved after batch {batch_idx + 1}\")\n\n    print(\"Evaluate model\")\n    loss, mae = model.evaluate(X_test, y_test)\n    print(f'Batch {batch_idx + 1} - Test Loss: {loss}, Test MAE: {mae}')\n\n    # Clear the TensorFlow session to release memory\n    K.clear_session()\n\n    # Trigger garbage collection to free up unused memory\n    gc.collect()\n\n    # Clear variables (X and y data) that are no longer needed\n    X = []\n    y = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T05:12:02.951134Z","iopub.execute_input":"2025-02-05T05:12:02.951444Z","iopub.status.idle":"2025-02-05T05:21:45.464020Z","shell.execute_reply.started":"2025-02-05T05:12:02.951413Z","shell.execute_reply":"2025-02-05T05:21:45.462647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nimport requests\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, LSTM, TimeDistributed, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom tqdm import tqdm\nimport tensorflow.keras.backend as K\nimport gc\n\n# Load pre-trained CNN model (feature extractor)\ndef create_grayscale_resnet():\n    # Use a custom input layer for grayscale images (1 channel)\n    input_tensor = Input(shape=(224, 224, 1))\n    \n    # Replicate the single channel to match ResNet50's expected input (3 channels)\n    replicated_input = tf.keras.layers.Concatenate()([input_tensor] * 3)\n    \n    # Load ResNet50 with replicated input\n    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=replicated_input)\n    base_model.trainable = False  # Freeze pre-trained layers\n    \n    # Add a feature extraction layer\n    feature_extractor = Model(inputs=input_tensor, outputs=Flatten()(base_model.output))\n    return feature_extractor\n\nfeature_extractor = create_grayscale_resnet()\n\n# Define the LSTM-based regression model\nmodel = Sequential([\n    TimeDistributed(feature_extractor, input_shape=(5, 224, 224, 1)),  # Grayscale input\n    LSTM(256, return_sequences=False),  # LSTM processes extracted features\n    Dense(512, activation='relu'),\n    Dropout(0.3),\n    Dense(3)  # Predict ExitVelocity, HitDistance, LaunchAngle\n])\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\nmodel.summary()\n\n# Function to download video\ndef download_video(video_url, save_path):\n    response = requests.get(video_url, stream=True)\n    with open(save_path, 'wb') as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            file.write(chunk)\n    return save_path\n\n# Function to detect ball-and-bat collision\ndef detect_collision(cap):\n    \"\"\"\n    Detects the frame where ball-and-bat collision occurs.\n    This is a placeholder implementation; you may need to use object detection or motion analysis.\n    \"\"\"\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Start from the beginning\n    prev_frame = None\n    collision_frame_id = 0\n\n    for frame_id in range(total_frames):\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Convert frame to grayscale for motion detection\n        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        gray_frame = cv2.GaussianBlur(gray_frame, (21, 21), 0)\n\n        if prev_frame is None:\n            prev_frame = gray_frame\n            continue\n\n        # Compute absolute difference between current and previous frame\n        frame_diff = cv2.absdiff(prev_frame, gray_frame)\n        _, thresh = cv2.threshold(frame_diff, 30, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # If significant motion is detected, assume collision\n        if any(cv2.contourArea(contour) > 500 for contour in contours):  # Adjust threshold as needed\n            collision_frame_id = frame_id\n            break\n\n        prev_frame = gray_frame\n\n    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset to the beginning\n    return collision_frame_id\n\n# Function to extract frames from video\ndef preprocess_video(video_url, save_path, frame_count=5):\n    video_path = download_video(video_url, f'{save_path}.mp4')\n    cap = cv2.VideoCapture(video_path)\n\n    # Detect ball-and-bat collision\n    collision_frame_id = detect_collision(cap)\n    # print(f\"Detected collision at frame {collision_frame_id}\")\n\n    # Extract frames starting from the collision frame\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.set(cv2.CAP_PROP_POS_FRAMES, collision_frame_id)  # Start from collision frame\n\n    for _ in range(frame_count):\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Convert frame to grayscale\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        # Resize frame to 224x224\n        frame = cv2.resize(frame, (224, 224))\n\n        # Quantize pixel values to integers (0-255)\n        frame = np.uint8(frame)\n\n        # Keep the frame as grayscale (1 channel)\n        frame = np.expand_dims(frame, axis=-1)  # Shape: (224, 224, 1)\n\n        frames.append(frame)\n\n    cap.release()\n    os.remove(video_path)  # Remove temp file\n    return np.array(frames) if len(frames) == frame_count else None\n\n# Load and merge datasets\ndf1 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2016-mlb-homeruns.csv')\ndf2 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2017-mlb-homeruns.csv')\ndf3 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2024-mlb-homeruns.csv')\ndf = pd.concat([df1, df2, df3], ignore_index=True)\n\n# Prepare training data\nbatch_size = 30\nnum_batches = len(df) // batch_size + (len(df) % batch_size != 0)\n\n# Create temp directory for videos\nos.makedirs(\"temp_videos\", exist_ok=True)\n\nfor batch_idx in range(num_batches):\n    print(f\"Processing batch {batch_idx + 1}/{num_batches}\")\n    df_batch = df[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n    X, y = [], []\n\n    # Process videos with proper tqdm progress tracking\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        futures = {executor.submit(preprocess_video, row['video'], 'temp_videos/' + row['play_id']): row for _, row in df_batch.iterrows()}\n\n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Videos\"):\n            res = future.result()\n            if res is not None:\n                X.append(res)\n                y.append([futures[future]['ExitVelocity'], futures[future]['HitDistance'], futures[future]['LaunchAngle']])\n\n    if len(X) == 0:\n        print(\"Skipping batch due to no valid videos.\")\n        continue\n\n    X = np.array(X)  # Shape: (batch_size, 5, 224, 224, 1)\n    y = np.array(y)  # Shape: (batch_size, 3)\n\n    print(\"Split data into train and test\")\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    print(\"Train model\")\n    model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test, y_test))\n\n    print(\"Deleting Previous Saved model\")\n    if os.path.exists(f'model_batch_{batch_idx}.h5'):\n        os.remove(f'model_batch_{batch_idx}.h5')\n\n    print(\"Save model after each batch\")\n    model.save(f'model_batch_{batch_idx + 1}.h5')\n    print(f\"Model saved after batch {batch_idx + 1}\")\n\n    print(\"Evaluate model\")\n    loss, mae = model.evaluate(X_test, y_test)\n    print(f'Batch {batch_idx + 1} - Test Loss: {loss}, Test MAE: {mae}')\n\n    # Clear the TensorFlow session to release memory\n    K.clear_session()\n\n    # Trigger garbage collection to free up unused memory\n    gc.collect()\n\n    # Clear variables (X and y data) that are no longer needed\n    X = []\n    y = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T05:30:34.306076Z","iopub.execute_input":"2025-02-05T05:30:34.306436Z","iopub.status.idle":"2025-02-05T05:39:43.320724Z","shell.execute_reply.started":"2025-02-05T05:30:34.306408Z","shell.execute_reply":"2025-02-05T05:39:43.319461Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"MobileNet CNN & LSTM","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nimport requests\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, LSTM, TimeDistributed, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom tqdm import tqdm\nimport tensorflow.keras.backend as K\nimport gc\n\n# Load pre-trained CNN model (feature extractor)\ndef create_grayscale_mobilenet():\n    # Use a custom input layer for grayscale images (1 channel)\n    input_tensor = Input(shape=(224, 224, 1))\n    \n    # Replicate the single channel to match MobileNet's expected input (3 channels)\n    replicated_input = tf.keras.layers.Concatenate()([input_tensor] * 3)\n    \n    # Load MobileNetV2 with replicated input\n    base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=replicated_input, pooling='avg')\n    base_model.trainable = False  # Freeze pre-trained layers\n    \n    # Add a feature extraction layer\n    feature_extractor = Model(inputs=input_tensor, outputs=base_model.output)\n    return feature_extractor\n\nfeature_extractor = create_grayscale_mobilenet()\n\n# Define the LSTM-based regression model\nmodel = Sequential([\n    TimeDistributed(feature_extractor, input_shape=(5, 224, 224, 1)),  # Grayscale input\n    LSTM(256, return_sequences=False),  # LSTM processes extracted features\n    Dense(512, activation='relu'),\n    Dropout(0.3),\n    Dense(3)  # Predict ExitVelocity, HitDistance, LaunchAngle\n])\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\nmodel.summary()\n\n# Function to download video\ndef download_video(video_url, save_path):\n    response = requests.get(video_url, stream=True)\n    with open(save_path, 'wb') as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            file.write(chunk)\n    return save_path\n\n# Function to detect ball-and-bat collision\ndef detect_collision(cap):\n    \"\"\"\n    Detects the frame where ball-and-bat collision occurs.\n    This is a placeholder implementation; you may need to use object detection or motion analysis.\n    \"\"\"\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Start from the beginning\n    prev_frame = None\n    collision_frame_id = 0\n\n    for frame_id in range(total_frames):\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Convert frame to grayscale for motion detection\n        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        gray_frame = cv2.GaussianBlur(gray_frame, (21, 21), 0)\n\n        if prev_frame is None:\n            prev_frame = gray_frame\n            continue\n\n        # Compute absolute difference between current and previous frame\n        frame_diff = cv2.absdiff(prev_frame, gray_frame)\n        _, thresh = cv2.threshold(frame_diff, 30, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # If significant motion is detected, assume collision\n        if any(cv2.contourArea(contour) > 500 for contour in contours):  # Adjust threshold as needed\n            collision_frame_id = frame_id\n            break\n\n        prev_frame = gray_frame\n\n    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset to the beginning\n    return collision_frame_id\n\n# Function to extract frames from video\ndef preprocess_video(video_url, save_path, frame_count=5):\n    video_path = download_video(video_url, f'{save_path}.mp4')\n    cap = cv2.VideoCapture(video_path)\n\n    # Detect ball-and-bat collision\n    collision_frame_id = detect_collision(cap)\n    #print(f\"Detected collision at frame {collision_frame_id}\")\n\n    # Extract frames starting from the collision frame\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.set(cv2.CAP_PROP_POS_FRAMES, collision_frame_id)  # Start from collision frame\n\n    for _ in range(frame_count):\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Convert frame to grayscale\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        # Resize frame to 224x224\n        frame = cv2.resize(frame, (224, 224))\n\n        # Quantize pixel values to integers (0-255)\n        frame = np.uint8(frame)\n\n        # Keep the frame as grayscale (1 channel)\n        frame = np.expand_dims(frame, axis=-1)  # Shape: (224, 224, 1)\n\n        frames.append(frame)\n\n    cap.release()\n    os.remove(video_path)  # Remove temp file\n    return np.array(frames) if len(frames) == frame_count else None\n\n# Load and merge datasets\ndf1 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2016-mlb-homeruns.csv')\ndf2 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2017-mlb-homeruns.csv')\ndf3 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2024-mlb-homeruns.csv')\ndf = pd.concat([df1, df2, df3], ignore_index=True)\n\n# Prepare training data\nbatch_size = 30\nnum_batches = len(df) // batch_size + (len(df) % batch_size != 0)\n\n# Create temp directory for videos\nos.makedirs(\"temp_videos\", exist_ok=True)\n\nfor batch_idx in range(num_batches):\n    print(f\"Processing batch {batch_idx + 1}/{num_batches}\")\n    df_batch = df[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n    X, y = [], []\n\n    # Process videos with proper tqdm progress tracking\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        futures = {executor.submit(preprocess_video, row['video'], 'temp_videos/' + row['play_id']): row for _, row in df_batch.iterrows()}\n\n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Videos\"):\n            res = future.result()\n            if res is not None:\n                X.append(res)\n                y.append([futures[future]['ExitVelocity'], futures[future]['HitDistance'], futures[future]['LaunchAngle']])\n\n    if len(X) == 0:\n        print(\"Skipping batch due to no valid videos.\")\n        continue\n\n    X = np.array(X)  # Shape: (batch_size, 5, 224, 224, 1)\n    y = np.array(y)  # Shape: (batch_size, 3)\n\n    print(\"Split data into train and test\")\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    print(\"Train model\")\n    model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test, y_test))\n\n    print(\"Deleting Previous Saved model\")\n    if os.path.exists(f'model_batch_{batch_idx}.h5'):\n        os.remove(f'model_batch_{batch_idx}.h5')\n\n    print(\"Save model after each batch\")\n    model.save(f'model_batch_{batch_idx + 1}.h5')\n    print(f\"Model saved after batch {batch_idx + 1}\")\n\n    print(\"Evaluate model\")\n    loss, mae = model.evaluate(X_test, y_test)\n    print(f'Batch {batch_idx + 1} - Test Loss: {loss}, Test MAE: {mae}')\n\n    # Clear the TensorFlow session to release memory\n    K.clear_session()\n\n    # Trigger garbage collection to free up unused memory\n    gc.collect()\n\n    # Clear variables (X and y data) that are no longer needed\n    X = []\n    y = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T11:09:59.816507Z","iopub.execute_input":"2025-02-05T11:09:59.816826Z","iopub.status.idle":"2025-02-05T11:12:55.053826Z","shell.execute_reply.started":"2025-02-05T11:09:59.816803Z","shell.execute_reply":"2025-02-05T11:12:55.052451Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-2-74d2357d0100>:26: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n  base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=replicated_input, pooling='avg')\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ time_distributed_1 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m)             │       \u001b[38;5;34m2,257,984\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m1,573,888\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m131,584\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │           \u001b[38;5;34m1,539\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ time_distributed_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,573,888</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,964,995\u001b[0m (15.13 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,964,995</span> (15.13 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,707,011\u001b[0m (6.51 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,707,011</span> (6.51 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Processing batch 1/549\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 30/30 [00:16<00:00,  1.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3s/step - loss: 55692.4062 - mae: 172.3088 - val_loss: 54941.0156 - val_mae: 173.3325\nEpoch 2/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 55112.4219 - mae: 171.6696 - val_loss: 52848.3398 - val_mae: 169.2892\nEpoch 3/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 51337.2578 - mae: 164.3454 - val_loss: 50501.9492 - val_mae: 164.4281\nEpoch 4/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 48801.4922 - mae: 157.6341 - val_loss: 47893.1055 - val_mae: 158.6192\nEpoch 5/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 48506.5703 - mae: 157.2095 - val_loss: 44998.0508 - val_mae: 151.7139\nEpoch 6/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 42377.3359 - mae: 143.5951 - val_loss: 41887.7812 - val_mae: 143.7994\nEpoch 7/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 41270.5273 - mae: 140.1787 - val_loss: 38611.7734 - val_mae: 135.7182\nEpoch 8/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 36837.1406 - mae: 129.7517 - val_loss: 35183.3945 - val_mae: 128.2468\nEpoch 9/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 35698.3750 - mae: 128.4760 - val_loss: 31630.8652 - val_mae: 121.1430\nEpoch 10/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 30262.0879 - mae: 116.9290 - val_loss: 28055.8438 - val_mae: 113.1143\nDeleting Previous Saved model\nSave model after each batch\nModel saved after batch 1\nEvaluate model\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 28055.8438 - mae: 113.1143\nBatch 1 - Test Loss: 28055.84375, Test MAE: 113.11431884765625\nProcessing batch 2/549\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 30/30 [00:16<00:00,  1.81it/s]","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - loss: 29188.8809 - mae: 114.7719 - val_loss: 23126.6816 - val_mae: 98.1742\nEpoch 2/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 25534.5352 - mae: 104.7664 - val_loss: 19780.9531 - val_mae: 87.3485\nEpoch 3/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 21498.5273 - mae: 92.7339 - val_loss: 16590.3594 - val_mae: 78.0670\nEpoch 4/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 18751.8984 - mae: 83.8353 - val_loss: 13641.6904 - val_mae: 73.3686\nEpoch 5/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 14565.6279 - mae: 77.4492 - val_loss: 10999.7773 - val_mae: 69.1487\nEpoch 6/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 12558.9277 - mae: 73.1239 - val_loss: 8669.0098 - val_mae: 64.5638\nEpoch 7/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 9509.6455 - mae: 67.3970 - val_loss: 6609.8008 - val_mae: 59.3165\nEpoch 8/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 7334.3027 - mae: 60.5917 - val_loss: 4827.2754 - val_mae: 52.3942\nEpoch 9/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5486.7124 - mae: 53.0084 - val_loss: 3337.6191 - val_mae: 44.3543\nEpoch 10/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4157.4419 - mae: 46.4784 - val_loss: 2145.8669 - val_mae: 35.3957\nDeleting Previous Saved model\nSave model after each batch\nModel saved after batch 2\nEvaluate model\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 2145.8669 - mae: 35.3957\nBatch 2 - Test Loss: 2145.866943359375, Test MAE: 35.39566421508789\nProcessing batch 3/549\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 30/30 [00:14<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - loss: 2262.4102 - mae: 34.9078 - val_loss: 1410.1708 - val_mae: 26.9067\nEpoch 2/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 1297.6450 - mae: 25.4838 - val_loss: 758.0928 - val_mae: 19.0779\nEpoch 3/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 923.9318 - mae: 21.2358 - val_loss: 370.9594 - val_mae: 12.7062\nEpoch 4/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 669.8268 - mae: 17.8879 - val_loss: 184.6286 - val_mae: 9.1787\nEpoch 5/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 270.2891 - mae: 11.6668 - val_loss: 132.6253 - val_mae: 8.4865\nEpoch 6/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 414.5684 - mae: 14.1541 - val_loss: 146.8019 - val_mae: 8.4977\nEpoch 7/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 346.2119 - mae: 13.4183 - val_loss: 181.9304 - val_mae: 9.4456\nEpoch 8/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 320.3073 - mae: 12.5182 - val_loss: 204.8627 - val_mae: 9.8685\nEpoch 9/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 581.7135 - mae: 18.6124 - val_loss: 202.3171 - val_mae: 9.7180\nEpoch 10/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 484.9227 - mae: 14.9706 - val_loss: 186.7753 - val_mae: 9.0021\nDeleting Previous Saved model\nSave model after each batch\nModel saved after batch 3\nEvaluate model\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 186.7753 - mae: 9.0021\nBatch 3 - Test Loss: 186.7753143310547, Test MAE: 9.002134323120117\nProcessing batch 4/549\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 30/30 [00:15<00:00,  1.88it/s]","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - loss: 8139.6006 - mae: 33.9802 - val_loss: 9720.8008 - val_mae: 34.7015\nEpoch 2/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 6652.9961 - mae: 29.3821 - val_loss: 9245.1689 - val_mae: 32.5466\nEpoch 3/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 6092.0547 - mae: 27.4804 - val_loss: 8666.8496 - val_mae: 30.1761\nEpoch 4/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 9715.3066 - mae: 34.7714 - val_loss: 8105.7993 - val_mae: 27.6794\nEpoch 5/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 5460.6870 - mae: 27.1557 - val_loss: 7693.5181 - val_mae: 27.3970\nEpoch 6/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 3982.3059 - mae: 24.6010 - val_loss: 7397.5757 - val_mae: 28.3118\nEpoch 7/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4476.2939 - mae: 28.0113 - val_loss: 7172.2656 - val_mae: 30.0979\nEpoch 8/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 5918.8838 - mae: 32.7680 - val_loss: 7016.2261 - val_mae: 31.4878\nEpoch 9/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 5143.6172 - mae: 30.9656 - val_loss: 6922.5220 - val_mae: 32.4658\nEpoch 10/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 7297.6523 - mae: 38.5864 - val_loss: 6859.1328 - val_mae: 33.3146\nDeleting Previous Saved model\nSave model after each batch\nModel saved after batch 4\nEvaluate model\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 6859.1328 - mae: 33.3146\nBatch 4 - Test Loss: 6859.1328125, Test MAE: 33.31463623046875\nProcessing batch 5/549\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 30/30 [00:17<00:00,  1.75it/s]","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - loss: 2464.4436 - mae: 25.8492 - val_loss: 1434.3341 - val_mae: 21.7198\nEpoch 2/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 2808.0586 - mae: 27.1151 - val_loss: 1372.1949 - val_mae: 21.3111\nEpoch 3/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 1857.8444 - mae: 25.7752 - val_loss: 1231.9730 - val_mae: 20.2302\nEpoch 4/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 3540.2783 - mae: 28.4844 - val_loss: 1082.5768 - val_mae: 19.4149\nEpoch 5/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 1910.8398 - mae: 23.5083 - val_loss: 907.9352 - val_mae: 18.2710\nEpoch 6/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 3356.8706 - mae: 26.5617 - val_loss: 777.8880 - val_mae: 17.3100\nEpoch 7/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 2371.5488 - mae: 23.1532 - val_loss: 652.3516 - val_mae: 16.1891\nEpoch 8/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 1607.2435 - mae: 19.7073 - val_loss: 548.9636 - val_mae: 15.0407\nEpoch 9/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 1983.7156 - mae: 18.4778 - val_loss: 485.4570 - val_mae: 14.3090\nEpoch 10/10\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 1693.6509 - mae: 18.4233 - val_loss: 442.5574 - val_mae: 13.6615\nDeleting Previous Saved model\nSave model after each batch\nModel saved after batch 5\nEvaluate model\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 442.5574 - mae: 13.6615\nBatch 5 - Test Loss: 442.5574035644531, Test MAE: 13.661486625671387\nProcessing batch 6/549\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos:  50%|█████     | 15/30 [00:09<00:09,  1.62it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-74d2357d0100>\u001b[0m in \u001b[0;36m<cell line: 143>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp_videos/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'play_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Processing Videos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    243\u001b[0m                             len(pending), total_futures))\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nimport requests\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, LSTM, TimeDistributed, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom tqdm import tqdm\nimport tensorflow.keras.backend as K\nimport gc\n\n# Load pre-trained CNN model (feature extractor)\ndef create_grayscale_mobilenet():\n    input_tensor = Input(shape=(128, 128, 1))  # Updated resolution to 128x128\n    replicated_input = tf.keras.layers.Concatenate()([input_tensor] * 3)\n\n    base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=replicated_input, pooling='avg')\n    base_model.trainable = False  \n\n    feature_extractor = Model(inputs=input_tensor, outputs=base_model.output)\n    return feature_extractor\n\nfeature_extractor = create_grayscale_mobilenet()\n\n# Define the LSTM-based regression model\nmodel = Sequential([\n    TimeDistributed(feature_extractor, input_shape=(5, 128, 128, 1)),  # Updated resolution\n    LSTM(256, return_sequences=False),  \n    Dense(512, activation='relu'),\n    Dropout(0.3),\n    Dense(3)  \n])\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\nmodel.summary()\n\n# Function to download video\ndef download_video(video_url, save_path):\n    response = requests.get(video_url, stream=True)\n    with open(save_path, 'wb') as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            file.write(chunk)\n    return save_path\n\n# Function to detect ball-and-bat collision\ndef detect_collision(cap):\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  \n    prev_frame = None\n    collision_frame_id = 0\n\n    for frame_id in range(total_frames):\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        gray_frame = cv2.GaussianBlur(gray_frame, (21, 21), 0)\n\n        if prev_frame is None:\n            prev_frame = gray_frame\n            continue\n\n        frame_diff = cv2.absdiff(prev_frame, gray_frame)\n        _, thresh = cv2.threshold(frame_diff, 30, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        if any(cv2.contourArea(contour) > 500 for contour in contours):\n            collision_frame_id = frame_id\n            break\n\n        prev_frame = gray_frame\n\n    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  \n    return collision_frame_id\n\n# Function to extract frames from video\ndef preprocess_video(video_url, save_path, frame_count=5):\n    video_path = download_video(video_url, f'{save_path}.mp4')\n    cap = cv2.VideoCapture(video_path)\n\n    collision_frame_id = detect_collision(cap)\n\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.set(cv2.CAP_PROP_POS_FRAMES, collision_frame_id)  \n\n    for _ in range(frame_count):\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        frame = cv2.resize(frame, (128, 128))  # Updated resolution\n\n        frame = np.uint8(frame)  \n\n        frame = np.expand_dims(frame, axis=-1)  \n\n        frames.append(frame)\n\n    cap.release()\n    os.remove(video_path)  \n    return np.array(frames, dtype=np.float16) if len(frames) == frame_count else None  # Use float16 to reduce size\n\n# Load and merge datasets\ndf1 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2016-mlb-homeruns.csv')\ndf2 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2017-mlb-homeruns.csv')\ndf3 = pd.read_csv('/kaggle/input/mlb-fan-content-interaction/2024-mlb-homeruns.csv')\ndf = pd.concat([df1, df2, df3], ignore_index=True)\n\nbatch_size = 16  # Reduced batch size for memory efficiency\nnum_batches = len(df) // batch_size + (len(df) % batch_size != 0)\n\nos.makedirs(\"temp_videos\", exist_ok=True)\n\nfor batch_idx in range(num_batches):\n    print(f\"Processing batch {batch_idx + 1}/{num_batches}\")\n    df_batch = df[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n    X, y = [], []\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        futures = {executor.submit(preprocess_video, row['video'], 'temp_videos/' + row['play_id']): row for _, row in df_batch.iterrows()}\n\n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Videos\"):\n            res = future.result()\n            if res is not None:\n                X.append(res)\n                y.append([futures[future]['ExitVelocity'], futures[future]['HitDistance'], futures[future]['LaunchAngle']])\n\n    if len(X) == 0:\n        print(\"Skipping batch due to no valid videos.\")\n        continue\n\n    X = np.array(X, dtype=np.float16)  \n    y = np.array(y, dtype=np.float16)  \n\n    print(\"Split data into train and test\")\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    print(\"Train model\")\n    model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test, y_test))\n\n    print(\"Deleting Previous Saved model\")\n    if os.path.exists(f'model_batch_{batch_idx}.h5'):\n        os.remove(f'model_batch_{batch_idx}.h5')\n\n    print(\"Save model after each batch\")\n    model.save(f'model_batch_{batch_idx + 1}.h5')\n    print(f\"Model saved after batch {batch_idx + 1}\")\n\n    print(\"Evaluate model\")\n    loss, mae = model.evaluate(X_test, y_test)\n    print(f'Batch {batch_idx + 1} - Test Loss: {loss}, Test MAE: {mae}')\n\n    K.clear_session()\n    gc.collect()\n\n    X = []\n    y = []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T16:45:50.248775Z","iopub.execute_input":"2025-02-05T16:45:50.249071Z","iopub.status.idle":"2025-02-05T16:48:28.608764Z","shell.execute_reply.started":"2025-02-05T16:45:50.249040Z","shell.execute_reply":"2025-02-05T16:48:28.607295Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-1-166f508ce789>:22: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n  base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=replicated_input, pooling='avg')\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ time_distributed (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m)             │       \u001b[38;5;34m2,257,984\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m1,573,888\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m131,584\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │           \u001b[38;5;34m1,539\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ time_distributed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,573,888</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,964,995\u001b[0m (15.13 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,964,995</span> (15.13 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,707,011\u001b[0m (6.51 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,707,011</span> (6.51 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Processing batch 1/1028\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 16/16 [00:09<00:00,  1.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 6s/step - loss: 52356.7109 - mae: 164.7740 - val_loss: 56495.4023 - val_mae: 175.0296\nEpoch 2/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 51063.4961 - mae: 162.9941 - val_loss: 55215.0039 - val_mae: 172.8725\nEpoch 3/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 50348.3516 - mae: 161.7522 - val_loss: 53865.1953 - val_mae: 170.4616\nEpoch 4/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 48486.3867 - mae: 158.7470 - val_loss: 52402.4492 - val_mae: 167.6902\nEpoch 5/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 49657.6289 - mae: 162.0238 - val_loss: 50820.4727 - val_mae: 164.5478\nEpoch 6/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 45479.6367 - mae: 153.1887 - val_loss: 49158.5938 - val_mae: 161.0701\nEpoch 7/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 45095.3398 - mae: 151.2985 - val_loss: 47413.9805 - val_mae: 157.2306\nEpoch 8/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 44963.3203 - mae: 152.1163 - val_loss: 45586.5273 - val_mae: 153.0044\nEpoch 9/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 41076.8789 - mae: 143.5587 - val_loss: 43671.0586 - val_mae: 148.3597\nEpoch 10/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 39345.5977 - mae: 138.7837 - val_loss: 41650.5117 - val_mae: 143.2750\nDeleting Previous Saved model\nSave model after each batch\nModel saved after batch 1\nEvaluate model\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 41650.5117 - mae: 143.2750\nBatch 1 - Test Loss: 41650.51171875, Test MAE: 143.27503967285156\nProcessing batch 2/1028\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 16/16 [00:08<00:00,  1.93it/s]","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - loss: 42820.3906 - mae: 146.3812 - val_loss: 40480.5391 - val_mae: 140.8886\nEpoch 2/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 41029.4688 - mae: 141.5823 - val_loss: 38287.7109 - val_mae: 137.7424\nEpoch 3/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 38613.6016 - mae: 137.1265 - val_loss: 36030.4922 - val_mae: 134.3510\nEpoch 4/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 35547.8008 - mae: 132.1102 - val_loss: 33705.8086 - val_mae: 130.4583\nEpoch 5/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 34945.4297 - mae: 130.6359 - val_loss: 31324.0078 - val_mae: 125.9128\nEpoch 6/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 31183.5840 - mae: 123.7333 - val_loss: 28936.1934 - val_mae: 120.7286\nEpoch 7/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 29715.6797 - mae: 119.9134 - val_loss: 26550.2109 - val_mae: 114.9274\nEpoch 8/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 26250.1738 - mae: 112.5243 - val_loss: 24176.7227 - val_mae: 108.5435\nEpoch 9/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 23889.1641 - mae: 105.5016 - val_loss: 21826.2109 - val_mae: 101.6110\nEpoch 10/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 22025.5059 - mae: 100.5417 - val_loss: 19526.7070 - val_mae: 94.1751\nDeleting Previous Saved model\nSave model after each batch\nModel saved after batch 2\nEvaluate model\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 19526.7070 - mae: 94.1751\nBatch 2 - Test Loss: 19526.70703125, Test MAE: 94.17511749267578\nProcessing batch 3/1028\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s]","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - loss: 20072.2637 - mae: 92.1895 - val_loss: 17315.0234 - val_mae: 84.7767\nEpoch 2/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 18867.3066 - mae: 88.0854 - val_loss: 15180.1738 - val_mae: 76.5039\nEpoch 3/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 16143.5029 - mae: 78.9099 - val_loss: 13196.5820 - val_mae: 69.6638\nEpoch 4/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 14016.4697 - mae: 71.9589 - val_loss: 11372.9492 - val_mae: 64.7760\nEpoch 5/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 12711.5674 - mae: 70.0420 - val_loss: 9709.1016 - val_mae: 61.9339\nEpoch 6/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 9216.5020 - mae: 61.2592 - val_loss: 8204.8916 - val_mae: 58.9487\nEpoch 7/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 8897.3115 - mae: 61.4052 - val_loss: 6840.9487 - val_mae: 55.7569\nEpoch 8/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 7491.4355 - mae: 58.9160 - val_loss: 5617.5215 - val_mae: 52.1431\nEpoch 9/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 5717.2163 - mae: 52.3138 - val_loss: 4532.7178 - val_mae: 47.9938\nEpoch 10/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 4663.7275 - mae: 48.9180 - val_loss: 3577.6489 - val_mae: 43.3586\nDeleting Previous Saved model\nSave model after each batch\nModel saved after batch 3\nEvaluate model\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 3577.6489 - mae: 43.3586\nBatch 3 - Test Loss: 3577.64892578125, Test MAE: 43.358612060546875\nProcessing batch 4/1028\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 16/16 [00:08<00:00,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - loss: 2761.2588 - mae: 38.9594 - val_loss: 1987.5338 - val_mae: 34.3815\nEpoch 2/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 2129.7915 - mae: 35.6637 - val_loss: 1432.6206 - val_mae: 29.4237\nEpoch 3/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1502.5410 - mae: 29.3201 - val_loss: 995.0404 - val_mae: 24.8138\nEpoch 4/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 1101.4120 - mae: 25.7612 - val_loss: 660.7618 - val_mae: 20.1829\nEpoch 5/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 708.3522 - mae: 21.4106 - val_loss: 420.4753 - val_mae: 16.2772\nEpoch 6/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 921.7455 - mae: 22.4866 - val_loss: 259.3071 - val_mae: 13.2057\nEpoch 7/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 404.8443 - mae: 14.7550 - val_loss: 166.2091 - val_mae: 10.1111\nEpoch 8/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 368.5130 - mae: 14.0866 - val_loss: 126.5624 - val_mae: 7.8954\nEpoch 9/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 569.3726 - mae: 17.3231 - val_loss: 124.6110 - val_mae: 7.5005\nEpoch 10/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 159.5224 - mae: 9.8829 - val_loss: 145.6389 - val_mae: 8.1431\nDeleting Previous Saved model\nSave model after each batch\nModel saved after batch 4\nEvaluate model\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 145.6389 - mae: 8.1431\nBatch 4 - Test Loss: 145.63888549804688, Test MAE: 8.143051147460938\nProcessing batch 5/1028\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 16/16 [00:07<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - loss: 399.2712 - mae: 15.1272 - val_loss: 55.3538 - val_mae: 6.2717\nEpoch 2/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 476.6289 - mae: 16.1240 - val_loss: 61.5768 - val_mae: 6.7578\nEpoch 3/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 568.9958 - mae: 17.0794 - val_loss: 64.7872 - val_mae: 6.8743\nEpoch 4/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 422.8990 - mae: 13.4494 - val_loss: 64.3739 - val_mae: 6.7110\nEpoch 5/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 392.2821 - mae: 12.7365 - val_loss: 63.5546 - val_mae: 6.8914\nEpoch 6/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 445.3592 - mae: 16.3313 - val_loss: 65.3956 - val_mae: 7.1934\nEpoch 7/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 487.4278 - mae: 16.1952 - val_loss: 61.3761 - val_mae: 6.9121\nEpoch 8/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 294.2959 - mae: 11.9596 - val_loss: 52.6391 - val_mae: 6.2033\nEpoch 9/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 517.7717 - mae: 15.2974 - val_loss: 44.4982 - val_mae: 5.3331\nEpoch 10/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 387.7775 - mae: 12.4551 - val_loss: 40.2881 - val_mae: 4.7769\nDeleting Previous Saved model\nSave model after each batch\nModel saved after batch 5\nEvaluate model\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 40.2881 - mae: 4.7769\nBatch 5 - Test Loss: 40.2880859375, Test MAE: 4.776850700378418\nProcessing batch 6/1028\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 16/16 [00:07<00:00,  2.27it/s]","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - loss: 380.9196 - mae: 13.1678 - val_loss: 147.4866 - val_mae: 7.2330\nEpoch 2/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 242.4663 - mae: 12.0816 - val_loss: 155.7843 - val_mae: 7.3532\nEpoch 3/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 186.3758 - mae: 10.1463 - val_loss: 164.3732 - val_mae: 7.4423\nEpoch 4/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 291.3875 - mae: 11.0587 - val_loss: 172.3923 - val_mae: 7.4455\nEpoch 5/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 485.0028 - mae: 14.1417 - val_loss: 179.1740 - val_mae: 7.4651\nEpoch 6/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 419.6431 - mae: 13.2852 - val_loss: 181.4986 - val_mae: 7.6942\nEpoch 7/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 255.8004 - mae: 11.9265 - val_loss: 182.8488 - val_mae: 7.9357\nEpoch 8/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 364.9121 - mae: 14.2854 - val_loss: 182.3547 - val_mae: 7.9517\nEpoch 9/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 406.6990 - mae: 13.8252 - val_loss: 182.3410 - val_mae: 7.8800\nEpoch 10/10\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 591.9685 - mae: 17.7831 - val_loss: 181.0927 - val_mae: 8.0051\nDeleting Previous Saved model\nSave model after each batch\nModel saved after batch 6\nEvaluate model\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 181.0927 - mae: 8.0051\nBatch 6 - Test Loss: 181.09274291992188, Test MAE: 8.005105018615723\nProcessing batch 7/1028\n","output_type":"stream"},{"name":"stderr","text":"Processing Videos: 100%|██████████| 16/16 [00:09<00:00,  1.60it/s]","output_type":"stream"},{"name":"stdout","text":"Split data into train and test\nTrain model\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-166f508ce789>\u001b[0m in \u001b[0;36m<cell line: 121>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Deleting Previous Saved model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1}]}